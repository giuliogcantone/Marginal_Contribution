---
title: "Untitled"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
pacman::p_load(
  tidyverse,
  openalexR,
  readxl,
  rvest,
  arrow
)
```

## Step 1: Find Most cited journal

SCOPUS

Scraping Scopus Categories

```{r}
read_html("https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus/") %>%
  html_elements(xpath="//td | //th") %>%
  html_text() %>%
  .[4:1005] -> Scopus_codes

tibble(
  Code = Scopus_codes[seq(1, length(Scopus_codes), by = 3)],
  Subject = Scopus_codes[seq(2, length(Scopus_codes), by = 3)],
  Field = Scopus_codes[seq(3, length(Scopus_codes), by = 3)]
  ) %>%
  mutate(
  Area = str_sub(Code, start = 1, end = 2) %>%
    case_match(
      "10" ~ "General",
      "11" ~ "Agricultural and Biological Sciences",
      "12" ~ "Arts and Humanities",
      "13" ~ "Biochemistry, Genetics and Molecular Biology",
      "14" ~ "Business, Management and Accounting",
      "15" ~ "Chemical Engineering",
      "16" ~ "Chemistry",
      "17" ~ "Computer Science",
      "18" ~ "Decision Sciences",
      "19" ~ "Earth and Planetary Sciences",
      "20" ~ "Economics, Econometrics and Finance",
      "21" ~ "Energy",
      "22" ~ "Engineering",
      "23" ~ "Environmental Science",
      "24" ~ "Immunology and Microbiology",
      "25" ~ "Materials Science",
      "26" ~ "Mathematics",
      "27" ~ "Medicine",
      "28" ~ "Neuroscience",
      "29" ~ "Nursing",
      "30" ~ "Pharmacology, Toxicology and Pharmaceutics",
      "31" ~ "Physics and Astronomy",
      "32" ~ "Psychology",
      "33" ~ "Social Sciences",
      "34" ~ "Veterinary",
      "35" ~ "Dentistry",
      "36" ~ "Health Professions"
  )
  ) -> Scopus_codes

Scopus_db <- read_excel("Scopus_db.xlsx") %>%
  transmute(ISSN = `Print-ISSN` %>% as.character(),
            eISSN = `E-ISSN` %>% as.character(),
            Title,
            Active = `Active or Inactive`,
            Type = `Source Type`,
            Scopus_Citescore = CiteScore %>% as.numeric(),
            Codes = `All Science Journal Classification Codes (ASJC)` %>%
              str_sub(end = -2)
) %>%
  filter(Type == "Journal",
         Active == "Active"
         ) %>%
  mutate(Scopus_Area =
           str_replace_all(Codes,
                           setNames(Scopus_codes$Area, Scopus_codes$Code)),
         Scopus_Area =
           map_chr(
             str_split(Scopus_Area, "; "),
             ~ str_c(unique(.x),
                     collapse = "; "))) %>%
  separate_rows(Scopus_Area, sep = "; ") -> Scopus_db


Scopus_db %>%
  arrange(Scopus_Area,
          -Scopus_Citescore) %>%
  group_by(Scopus_Area) %>%
  slice_head(n = 10) %>%
  .[9:nrow(.),] %>%
  pivot_longer(1:2,
               values_to = "ISSN") %>%
  distinct(ISSN, .keep_all = T) %>%
    select(-name) %>%
  filter(!ISSN %>% is.na()) %>%
  mutate(ISSN = ISSN %>% str_replace("(.{4})(.)",
                                     "\\1-\\2")) -> j_selection

```

## Retrieve bibliometric data by OpenAlex

```{r}
openalexR::oa_fetch(
  entity = "venues",
  issn = j_selection %>%
    pull(ISSN) %>% unique()
) -> j_sample

j_sample %>%
  arrange(-works_count) %>% .[2,] -> p


openalexR::oa_fetch(
  entity = "works",
  primary_location.source.id = j_sample %>%
    pull(id) %>% unique(),
  publication_year = c(2012,2017)
) -> sample

sample %>%
  distinct(id, .keep_all = T) -> sample
```

Micro-sample

```{r}
j_sample %>%
  arrange(-works_count) %>%
  head(5) %>%
  pull(id) %>%
  openalexR::oa_fetch(
  entity = "works",
  primary_location.source.id = .,
  publication_year = c(2012,2017)
) -> micro_sample
```

Pre processing

```{r}

sample %>%
  transmute(
    id,
    doi,
    Title = display_name,
    Journal = so,
    publication_date,
    publication_year,
    cited_by_count,
    counts_by_year,
    len = as.integer(last_page) - as.integer(first_page),
    len = ifelse(len < 0, 0, len),
    concepts
  ) %>%
  filter(!Title %>% is.na()) %>%
  filter(!Title %>% str_detect("Errat|Editor")) -> framed_sample

```

Hypothesis: we need to clean mono-page papers

```{r}

framed_sample %>%
  mutate(very_short = ifelse(len < 2, 1,0)) %>%
  summarise(
    n = n(),
    len = median(len),
    IF = median(cited_by_count),
            .by = very_short)

framed_sample %>%
  mutate(class_lenght =
           case_when(
             len < 2 ~ "micro",
             len %>% between(2,7) ~ "small",
             len > 7 ~ "big",
             T ~ "noinfo") %>%
           factor(levels = c("noinfo","micro","small","big"), ordered = T)
         ) %>%
  group_by(class_lenght) %>%
  summarise(
    n = n(),
    median_len = median(len+1),
    median_counts = median(cited_by_count),
    max_counts = max(cited_by_count)
    ) %>%
  gt::gt() %>%
  gt::gtsave("table.png")

```

Calculating impact

```{framed_sample %>%}
  unnest(counts_by_year,
         names_sep = "_") %>%
  summarize(
    FiveYear_Impact = sum(case_when(
      publication_year == 2017 ~ sum(counts_by_year_cited_by_count[
        between(counts_by_year_year, 2018, 2022)],
        na.rm = TRUE),
      publication_year == 2012 ~ sum(counts_by_year_cited_by_count[
        between(counts_by_year_year, 2013, 2017)],
        na.rm = TRUE))
    ),
    .by = id
  ) %>%
    right_join(framed_sample) -> framed_sample
```
